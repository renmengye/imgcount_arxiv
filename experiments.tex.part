\section{Experiments}

\textbf{CVPPP leaf segmentation}. One instance segmentation benchmark is the
CVPPP plant leaf dataset \cite{minervini14cvppp}, which was developed due to
the importance of instance segmentation in plant phenotyping. We ran the A1
subset of CVPPP plant leaf segmentation dataset. We trained our model on 128
labelled images, and report results on the 33 test images. We compare our
performance to \cite{romeraparedes15ris}, and other top approaches that were
published with the CVPPP conference; see the collation study
\cite{scharr16leaf} for details of these  other approaches.

\textbf{KITTI car segmentation}. Instance segmentation also provides rich
information in the context of autonomous driving. Following
\cite{zhang15insseg, zhang16insseg, uhrig16insseg}, we also evaluated the model
performance on KITTI car segmentation dataset. We trained the model with 3712
training images, and report performance on 144 test images. We also examine the
relative importance of model components via ablation studies:
\begin{itemize}
\item \textbf{No box net}. Instead of predicting segmentation within a box, the
output dimension of the segmentation network is the full image.
\item \textbf{No pre processing}. This network is trained to take raw image
pixels, without having foreground/angle map segmentation information.
\item \textbf{No scheduled sampling}. This network has the same architecture 
but trained without scheduled sampling (see Section~\ref{sec:postproc}).
\item \textbf{No post processing}. The same network but at test time we do not
apply post-processing techniques (see Section~\ref{sec:postproc})
\end{itemize}

\textbf{MS-COCO counting}. Additionally, we train class specific models on MS-
COCO and test counting performance on the results. We chose ``person'' and
``zebra'' because these are the two of the most common classes in VQA
questions. We report counting performance on images with at least one instance
of the class: 677 zebra images, and 21,634 ``person'' images.

\input{tabs/cvppp_table.tex.part}
\input{figs/cvppp_fig.tex.part}
\input{tabs/kitti_table.tex.part}
\input{figs/kitti_fig.tex.part}
\input{count_fig.tex.part}

\textbf{Evaluation metrics}. We report the  metrics used by the other studies
in the respective benchmarks: symmetric best dice (SBD) for leaf segmentation
(see Equations~\ref{eq:bd}, \ref{eq:sbd}) and mean (weighted) coverage (MWCov,
MUCov) for car segmentation (see Equations~\ref{eq:mwcov}, \ref{eq:mucov}). The
coverage scores measure the instance-wise IoU for each ground-truth instance
averaged over the image; MWCov further weights the score by the size of the
ground-truth instance segmentation (larger objects get larger weights).
\begin{align}
\label{eq:bd}
\texttt{DICE}(A, B)                &= \frac{2 |A \cup B|}{|A| + |B|}      \\
\texttt{BD}(\{A_i\}, B)            &= \max_{i} \texttt{DICE}(A_i, B)      \\
\label{eq:sbd}
\texttt{SBD}(y_i\}, \{y^*_j\})     &= \frac{1}{N} \min \left(
                                      \sum_j 
                                      \texttt{BD}(\{y_i\}, y^*_j),
                                      \sum_i
                                      \texttt{BD}(\{y^*_j\}, y_i) \right) \\
\label{eq:mucov}
\texttt{MUCov}(\{y_i\}, \{y_j^*\}) &= \frac{1}{N} \sum_i 
                                      \max_j \texttt{IoU}(y_i, y_j^*)     \\
\label{eq:mwcov}
\texttt{MWCov}(\{y_i\}, \{y_j^*\}) &= \frac{1}{N} \sum_i 
                                      \frac{|y_i|}{\sum_i |y_i|}
                                      \max_j \texttt{IoU}(y_i, y_j^*)
\end{align}

Counting is measured in absolute difference in count (\texttt{|DiC|}) (see
Equation~\ref{eq:dic}), average false positive (\texttt{AvgFP}), and average
false negative (\texttt{AvgFN}). False positive is the number of predicted
instances that do not overlap with the ground-truth, and false negative is the
number of  ground-truth instances that do not overlap with the prediction.
\begin{align}
\label{eq:dic}
\texttt{|DiC|} = \frac{1}{N}\sum_i |count_i - count_i^*|
\end{align}

\subsection{Results \& discussion}
In the leaf segmentation task, our best model outperforms the previous 
state-of-the-art by a large margin in both segmentation and counting.
Table~\ref{tab:cvppp} shows that the models with FCN overfit and 
scores lower than the simpler version. This is sensible as the dataset size
is small, and including the FCN significantly increases the input dimension and
number of parameters.

In the car segmentation task, our model achieves the state-of-the-art MWCov
shown in Table~\ref{tab:kitti}, but our MUCov is lower than results reported by
Uhrig et al. \cite{uhrig16insseg}. One possible explanation is their inclusion
of depth information during training, which may help the model disambiguate
distant object boundaries. Moreover, their bottom-up ``instance fusion'' method
plays a crucial role (omitting this leads to a steep performance drop); this
likely helps segment smaller objects, whereas our box network does not reliably
detect distant cars.

In the zebra counting task, we found that our model outperforms the detector
and NMS method, and associative-subitizing methods~\cite{chattopadhyay16count},
but we are not doing as well in the person category. However, relative to these
regression-based methods, our model permits insight into the recognition of
each instance by inspecting the output segmentation. Figure~\ref{fig:count}
shows the relation between counting performance and number of instances. Mean
absolute difference in count is around 1 for up to 18 leaves, 7 cars, 7 zebras
and 3 people.

In Figure \ref{fig:count}, instead of having a linear relation between counting
error and groundtruth count, as appeared in most regression based model, our
model is making much less errors in the low count regime in all four datasets,
reflecting the nature of our iterative counting method, which delivers higher
counting accuracy for small number of objects. Note that since our model is not
trained to directly output the total count, but instead jointly with pixel-
based loss function, it might give larger overall errors due to the model bias.

From Figures \ref{fig:cvppp_out}, \ref{fig:kitti_out},
\ref{fig:coco_person_out}, \ref{fig:coco_zebra_out} we see our model is
handling a significant amount of object occlusion and truncation. We verified
that the external memory helps with the counting process as the network first
segments the more salient objects and then accounts for the occluded instances.
In addition, our segmentation network can handle a range of object sizes
because of the design of the box network.

We found that using scheduled sampling results in much better performance. It
helps by making training resemble testing, gradually forcing the model to carry
out a full sequence during training instead of relying on ground-truth input.
Finally, the convolutional and attentional architecture significantly reduces
the number of parameters (around 9M) and the performance is quite strong
despite being trained with only 100 leaf images and 1000 zebra images.

\input{figs/coco_person_fig.tex.part}
\input{figs/coco_zebra_fig.tex.part}

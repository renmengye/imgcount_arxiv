\section{Experiments}

\textbf{CVPPP leaf segmentation}. One instance segmentation benchmark is the
CVPPP plant leaf dataset \cite{minervini14cvppp}, which was developed due to
the importance of instance segmentation in plant phenotyping. We ran the A1
subset of CVPPP plant leaf segmentation dataset. We trained our model on 128
labeled images, and report results on the 33 test images. We compare our
performance to \cite{romeraparedes15ris}, and other top approaches that were
published with the CVPPP conference; see the collation study
\cite{scharr16leaf} for details of these  other approaches.

\textbf{KITTI car segmentation}. Instance segmentation also provides rich
information in the context of autonomous driving. Following
\cite{zhang15insseg, zhang16insseg, uhrig16insseg}, we also evaluated the model
performance on KITTI car segmentation dataset. We trained the model with 3712
training images, and report performance on 144 test images. 

\textbf{MS-COCO images}. Additionally, we train class specific models on MS-
COCO and test counting performance on the results. We chose ``person'' and
``zebra'' because these are the two of the most common classes in VQA counting
questions. We report counting performance on images with at least one instance
of the class: 677 zebra images, and 21,634 ``person'' images.

\textbf{Ablation studies}. We also examine the relative importance of model
components via ablation studies:
\begin{itemize}
\item \textbf{No box net}. Instead of predicting segmentation within a box, the
output dimension of the segmentation network is the full image.
\item \textbf{No pre-processing}. This network is trained to take as input raw image
pixels, without the foreground segmentation or the angle map.
\item \textbf{No scheduled sampling}. This network has the same architecture 
but trained without scheduled sampling (see Section~\ref{sec:postproc}).
\item \textbf{No post-processing}. The same network but at test time we do not
apply post-processing techniques (see Section~\ref{sec:postproc})
\end{itemize}

\input{tabs/cvppp_table.tex.part}
\input{figs/cvppp_fig.tex.part}

\input{tabs/kitti_table.tex.part}
\input{figs/kitti_fig.tex.part}

\textbf{Evaluation metrics}. We evaluate based on the  metrics used by the other studies
in the respective benchmarks: symmetric best dice (SBD) for leaf segmentation
(see Equations~\ref{eq:bd}, \ref{eq:sbd}) and mean (weighted) coverage (MWCov,
MUCov) for car segmentation (see Equations~\ref{eq:mwcov}, \ref{eq:mucov}). The
coverage scores measure the instance-wise IoU for each ground-truth instance
averaged over the image; MWCov further weights the score by the size of the
ground-truth instance segmentation (larger objects get larger weights).
\begin{align}
\label{eq:bd}
\texttt{DICE}(A, B)                &= \frac{2 |A \cup B|}{|A| + |B|}      \\
\texttt{BD}(\{A_i\}, B)            &= \max_{i} \texttt{DICE}(A_i, B)      \\
\label{eq:sbd}
\texttt{SBD}(y_i\}, \{y^*_j\})     &= \frac{1}{N} \min \left(
                                      \sum_j 
                                      \texttt{BD}(\{y_i\}, y^*_j),
                                      \sum_i
                                      \texttt{BD}(\{y^*_j\}, y_i) \right) \\
\label{eq:mucov}
\texttt{MUCov}(\{y_i\}, \{y_j^*\}) &= \frac{1}{N} \sum_i 
                                      \max_j \texttt{IoU}(y_i, y_j^*)     \\
\label{eq:mwcov}
\texttt{MWCov}(\{y_i\}, \{y_j^*\}) &= \frac{1}{N} \sum_i 
                                      \frac{|y_i|}{\sum_i |y_i|}
                                      \max_j \texttt{IoU}(y_i, y_j^*)
\end{align}

Counting is measured in absolute difference in count (\texttt{|DiC|}) (see
Equation~\ref{eq:dic}), average false positive (\texttt{AvgFP}), and average
false negative (\texttt{AvgFN}). False positive is the number of predicted
instances that do not overlap with the ground-truth, and false negative is the
number of  ground-truth instances that do not overlap with the prediction.
\begin{align}
\label{eq:dic}
\texttt{|DiC|} = \frac{1}{N}\sum_i |count_i - count_i^*|
\end{align}

\subsection{Results \& Discussion}

Example results on the leaf segmentation task are shown in 
Figure \ref{fig:cvppp_out}.
On this task, our best model outperforms the previous 
state-of-the-art by a large margin in both segmentation and counting
(see Table~\ref{tab:cvppp}).
We found that the models with FCN overfit on this task, and we thus
utilized the simpler version without input pre-processing.
This is not surprising, as the dataset 
is very small, and including the FCN significantly increases the input dimension and
number of parameters.

In the KITTI task, Figure \ref{fig:kitti_out} shows that our
model can segment cars in a wide variety of poses. It achieves state-of-the-art 
results (see  Table~\ref{tab:kitti}) across several of the relevant measures,
including IoU, weighted coverage, and false positives.
Note however that our MUCov is lower than results reported by
Uhrig et al. \cite{uhrig16insseg}. One possible explanation is their inclusion
of depth information during training, which may help the model disambiguate
distant object boundaries. Moreover, their bottom-up ``instance fusion'' method
plays a crucial role (omitting this leads to a steep performance drop); this
likely helps segment smaller objects, whereas our box network does not reliably
detect distant cars.

In MS-COCO images, our model is able to learn to segment instances of the two
specific classes, zebra and humans, in some challenging instances.
In Figures \ref{fig:coco_person_out}, \ref{fig:coco_zebra_out} we see that our model is
handling a significant amount of object occlusion and truncation. We verified
that the external memory helps with the counting process as the network first
segments the more salient objects and then accounts for the occluded instances.
In addition, our segmentation network can handle a range of object sizes
because of the design of the box network.

With respect to the counting task,
Table \ref{tab:coco-count} shows
that our model outperforms the detector and NMS
method, and associative-subitizing methods~\cite{chattopadhyay16count} in the
zebra category, but not as well in the person category. It is not surprising
that a method trained directly to estimate the number of objects can
out-perform our model, which 
is trained to optimize the segmentation rather than the total count.
However, relative to
these regression-based methods, our model permits insight into the recognition
of each instance by inspecting the output segmentation. 

\input{figs/coco_person_fig.tex.part}
\input{figs/coco_zebra_fig.tex.part}
\input{count_fig.tex.part}

We obtain further insight into the counting task by examining Figure
\ref{fig:count} on the relation of counting error and the ground-truth count.
Instead of having a linear relation between the two, as is common in
regression-based models, our model is making many fewer errors in the low count
regime in all four datasets, reflecting the nature of our iterative counting
method.
This is arguably preferable, as errors in smaller counts are likely more
significant.

Finally, our ablation studies help elucidate the contribution of some
model components.
We found that using scheduled sampling results in much better performance. It
helps by making training resemble testing, gradually forcing the model to carry
out a full sequence during training instead of relying on ground-truth input.
Also, the convolutional and attentional architecture significantly reduces
the number of parameters (around 9M) and the performance is quite strong
despite being trained with only 100 leaf images and 1000 zebra images.
